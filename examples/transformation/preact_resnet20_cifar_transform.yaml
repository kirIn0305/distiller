
# time python3 compress_classifier.py -a preact_resnet20_cifar --lr 0.1 -p 50 -b 128 ../../../data.cifar10/ -j 1
# --epochs 200 --compress=../quantization/quant_aware_train/preact_resnet20_cifar_pact.yaml --out-dir="logs/" --wd=0.0002 --vs=0


#2018-07-18 17:28:56,710 - --- validate (epoch=199)-----------
#2018-07-18 17:28:56,710 - 10000 samples (128 per mini-batch)
#2018-07-18 17:28:58,070 - Epoch: [199][   50/   78]    Loss 0.349229    Top1 91.140625    Top5 99.671875
#2018-07-18 17:28:58,670 - ==> Top1: 91.440    Top5: 99.680    Loss: 0.348
#
#2018-07-18 17:28:58,671 - ==> Best validation Top1: 91.860   Epoch: 147
#2018-07-18 17:28:58,672 - Saving checkpoint to: logs/checkpoint.pth.tar
#2018-07-18 17:28:58,687 - --- test ---------------------
#2018-07-18 17:28:58,687 - 10000 samples (128 per mini-batch)
#2018-07-18 17:29:00,006 - Test: [   50/   78]    Loss 0.349229    Top1 91.140625    Top5 99.671875
#2018-07-18 17:29:00,560 - ==> Top1: 91.440    Top5: 99.680    Loss: 0.348


transformers:
  rl_transformer:
    class: ReplaceLayerTransformer
    transform_module: null
    transform_kernel: null
    transform_init_mode: null
    overrides:
      #layer1.0.conv1:
      layer2.1:
        transform_module:
          name: PathThrough
      #layer1.0.bn:
      #  transform_module:
      #    name: PathThrough
      #layer1.0.bn:
      #  transform_module:
      #    name: PathThrough

  ks_transformer:
    class: KernelSizeTransformer
    transform_module: null
    transform_kernel: null
    transform_init_mode: null
    overrides:
      conv1:
        transform_kernel: 7
      layer1.0.conv1:
        transform_kernel: 5

lr_schedulers:
  training_lr:
    class: MultiStepLR
    milestones: [60, 120]
    gamma: 0.1

policies:
    - transformer:
        instance_name: rl_transformer
      starting_epoch: 0
      ending_epoch: 200
      frequency: 1

    - transformer:
        instance_name: ks_transformer
      starting_epoch: 0
      ending_epoch: 200
      frequency: 1

    - lr_scheduler:
        instance_name: training_lr
      starting_epoch: 0
      ending_epoch: 121
      frequency: 1

